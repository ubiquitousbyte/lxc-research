\chapter{Fundamentals}
\section{Virtualisation}
Chapter \ref{ch:fundamentals/virtualisation/axioms} introduces the fundamental axioms that define 
virtualisation - noninterference, isolation and performance. 
Chapter \ref{ch:fundamentals/virtualisation/hardware-virtualisation} presents hardware virtualisation 
as a concept, discusses the mechanisms it uses to satisfy the axioms, and highlights the trade-offs
intrinsic to its design. 
Chapter \ref{ch:fundamentals/virtualisation/os-virtualisation} describes a more coarse-grained 
approach - operating system virtualisation - that has become the de-facto way of managing cloud workloads in 
multitenant environments. 

\subsection{Axioms}
\label{ch:fundamentals/virtualisation/axioms}
\subsubsection{Noninterference}
\label{ch:fundamentals/virtualisation/axioms/noninterference}
\textcite{10.1145/368481.368502} summarise the fundamental requirements of a multiprogramming 
system and emphasise the concept of noninterference between processes across space and time. 
\textit{Spatial noninterference} is represented by all mechanisms that protect references to memory, 
disk and input-output devices \cite{10.1145/368481.368502}. For example, memory segmentation 
is a method found in operating system kernels that assigns each process a dedicated portion
of physical memory that is invisible to all other processes in the system. The kernel traps 
any attempt made by a process to access memory outside its allocated memory segment, thereby 
guaranteeing spatial noninterference \cite{10.5555/2490781}. \textit{Temporal noninterference} refers
to those mechanisms that allocate execution time and protect against the monopolisation thereof 
\cite{10.1145/368481.368502}. For instance, CPU scheduling is a technique that decides which process 
shall run on a core such that the core does not idle and all processes make sufficient 
runtime progress \cite{10.5555/2490781}. The scheduling semantics, paired with an interrupt mechanism 
that makes sure that no process has hold of the core for too long, guarantee temporal noninterference.

\subsubsection{Isolation}
\label{ch:fundamentals/virtualisation/axioms/isolation}
\textcite{10.1145/3381052.3381315} define isolation as the level of dependency that a virtualisation 
platform has towards the host kernel. We generalise this definition and say that \textit{isolation} 
is the level of dependency that one piece of software has to another. Conceptually, isolation 
deals with explicit vertical relationships between software, and noninterference deals with 
implicit horizontal relationships between processes. Isolation can be quantified by counting the 
lines of external source code that a software executes to obtain a particular functionality. 
For example, \textcite{10.1145/3381052.3381315} count the lines of kernel code 
that a virtualisation platform executes when providing services to sandboxed applications. 
High counts indicate a strong dependency, i.e weak isolation, towards the kernel. 

\subsubsection{Performance}
\label{ch:fundamentals/virtualisation/axioms/performance}
\textcite{10.1145/3365199} defines performance as the contention between the overhead associated 
with isolating a process from its environment and the benefits of sharing resources between processes,
i.e fully utilising the capacity of the underlying resource pool. 
\textcite{10.1145/3381052.3381315} use the same definition and contrast the isolation mechanisms provided
by three different virtualisation platforms against processing unit, memory and input-output performance metrics. 
In particular, the authors define an application that computes prime numbers up to a limit. 
Since the workload is compute-bound, processing speed is measured and compared to the
number of executed lines of code that reside in the \verb|/arch|, \verb|/virt| and \verb|/arch/x86/kvm|
subsystems of the Linux kernel. \textcite{10.1145/3132747.3132763} use \textit{same-host density} as a 
performance metric that measures the number of sandboxed applications that can be consolidated onto a single server.
In addition, \textit{boot, pause and unpause times} are also considered to be important performance indicators for particular 
use cases, such as elastic content delivery networks \cite{10.1145/3050748.3050757} \cite{10.1145/3132747.3132763}
and serverless computing.

\subsection{Hardware Virtualisation}
\label{ch:fundamentals/virtualisation/hardware-virtualisation}
\textcite{10.1145/361011.361073} refer to the control program as a \textit{virtual machine monitor} that 
ensures isolation and noninterference by providing every program with an environment that is \enquote{[...] effect
identical with that demonstrated if the program had been run on the original machine directly} 
\cite[2]{10.1145/361011.361073}. This definition implies that a running program does not directly use
the bare metal resources available. Instead, resources are emulated by the virtual machine monitor at
the hardware level and presented as a dedicated physical system. Such an environment is called 
a \textit{virtual machine}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.70\textwidth]{images/fundamentals/full-virt-archh.jpg}
    \caption{Hardware virtualisation architecture. Each guest runs a complete operating system. 
             Privileged operations are trapped by the virtual machine monitor and emulated to provide hardware services.}
    \label{images:fundamentals/full-virt-archh.jpg}
\end{figure}

\textcite{10.1145/361011.361073} define a requirement that the instruction-set architecture of a computer
has to satisfy for it to be virtualisable. The instruction set must be segregated into three groups of
instructions - privileged, sensitive and innocuous. An instruction is privileged if it requires changing
the mode of execution from user to supervisor mode by means of a trap \cite{10.1145/361011.361073}. 
An instruction $i$ is control-sensitive if, when applied to the current processor state $S_1$, results
in a new state $i(S_{1}) = S_{2}$ such that the execution mode of $S_{2}$ does not equal that of $S_{1}$
or if $S_{2}$ has access to different resources than $S_1$ or both \cite{10.1145/361011.361073}. 
An instruction is behaviour-sensitive if its execution depends on the execution mode or its position
in memory \cite{10.1145/361011.361073}. An instruction is innocuous if it is not sensitive. 
Given these definitions, a computer is virtualisable \enquote{[...] if the set of sensitive instructions
for that computer is a subset of the set of privileged instructions} \cite[6]{10.1145/361011.361073}.
If this criterion is met, the virtual machine monitor can trap all sensitive instructions and emulate 
each via a homomorphism $i: C_{r} \rightarrow C_{v}$ that maps the state space of the processor without
the virtual machine monitor loaded $C_{r}$ to the state space with the virtual machine monitor loaded 
$C_{v}$ \cite{10.1145/361011.361073}. Innocuous instructions do not require protection, i.e a homomorphic
mapping, and are directly executed by the processor \cite{10.1145/361011.361073}.

Given the aforementioned homomorphism, a virtual machine can host a \textit{guest kernel} (Figure \ref{images:fundamentals/full-virt-archh.jpg}) that 
runs completely in user mode. 
Whenever the guest kernel attempts to execute a privileged instruction, 
the virtual machine monitor traps the attempt and emulates the instruction. 
Consequently, the guest kernel does not have to be a part of the trusted computing base. 
Even if it is compromised or encounters an unrecoverable error condition, other virtual machines 
remain unaffected. As a result, the isolation boundary between user programs running in different 
virtual machines is stronger compared to processes running on a shared kernel. 

In order to fully guarantee spatial noninterference between processes, the virtual machine monitor must 
be in full control of the host system's memory. There are two primary methods to do this - 
\textit{shadow paging} and \textit{extended page tables}. The former mechanism is considered first. 
The virtual machine monitor maintains a nested page table 
per guest, also called a \textit{shadow page table} \cite{10.5555/1204009}. 
In turn, the guest kernel maintains a page table per process. 
Whenever the guest kernel schedules a new process for execution, it modifies the \textit{page-table 
base register} to point to the page table for that process \cite{10.5555/1204009}. 
The virtual machine monitor intercepts this attempt and transparently updates the page table pointer to point to 
the guest's shadow page table corresponding to that process \cite{10.5555/1204009}. Note that 
the virtual machine monitor has to traverse the shadow page table for that guest in order to find the nested entry corresponding 
to the process. Afterwards, the memory management unit takes care of translating the virtual memory 
addresses of the guest and updating the \textit{translation lookaside buffer}.
Alternatively, the memory management unit may be \enquote{virtualisation-aware} in the sense that it knows 
there are two page tables it needs to traverse - the page table that maps guest virtual memory to guest 
\enquote{physical memory}, and the page table that maps guest physical memory to actual physical memory. 
The former is maintained by the guest kernel, whilst the latter is maintained by the virtual machine monitor.
The extended page table approach is up to 50\% faster than shadow paging \cite{2006PerformanceEO} because table
walks are done in hardware - by the memory management unit.
Nevertheless, maintaining page table data structures inside the virtual machine 
monitor and the guests leads to memory pressure, which is further amplified by the fact that 
guests, their applications and the virtual machine monitor all share the same physical memory \cite{10.5555/2490781}. 

The spatial noninterference property necessitates that the virtual machine monitor manage 
all input-output devices and their interactions with the guests. This is accomplished by the
already introduced trap-and-emulate pattern. When an application within a virtual machine 
issues a system call requesting some form of input-output, the request is processed by the 
I/O stack inside the guest. At the lowest level of the stack, the device driver issues a 
command to the device, typically by writing to memory specifically assigned to the device, or by
calling specific input-output instructions \cite{10.5555/2490781}. 
Either way, the virtual machine monitor intercepts this and traverses its own I/O stack, which 
remaps guest and real input-output addresses and forwards the request to a physical device \cite{10.1145/2063176.2063194}. 
After processing the request, the physical device triggers an interrupt that is caught by the virtual machine monitor and 
transformed into a virtual equivalent that is sent to the virtual machine that issued the request.
To reduce the overhead associated with interrupt processing, the virtual machine monitor can batch 
multiple events together and use a single interrupt to notify the guest kernel \cite{10.1145/2063176.2063194}.
Still, a request must traverse two input-output stacks. The same holds for the response.
In addition, hardware optimisations such as direct memory access are emulated in software, which 
further degrades performance. This, however, can be mitigated by integrating an input-output memory management 
unit that remaps all direct memory accesses of a device on the host to an address space in the guest.

The cost of hardware virtualisation becomes apparent when measuring same-host density
and boot times. \textcite{10.1145/3132747.3132763} consider memory consumption and on-disk image size
as the primary limiting factors. The authors measure the time it takes to create and boot
virtual machines using the Xen virtual machine monitor and show the negative effects that on-disk image size has 
by starting images with 
varying sizes by manually \enquote{[...] injecting binary objects into the uncompressed image file} \cite[3]{10.1145/3132747.3132763}. 
As the number of consolidated virtual instances increases and the image size grows, 
creation and boot times increase linearly.
Furthermore, the authors show that creating and starting a process directly on the host is, on average, 
two orders of magnitude faster. \textcite{10.1145/2151024.2151030} also evaluate Xen and state
that processing units spend 25\% of their total cycles in hypervisor mode instead of executing guest applications 
when running \enquote{[...] SPEC's first benchmark addressing performance evaluation of datacenter servers used in 
virtualised server consolidation} \cite[2]{10.1145/2151024.2151030}, which includes components 
such as a web, database and application server.

\subsection{Operating System Virtualisation}
\label{ch:fundamentals/virtualisation/os-virtualisation}
\textit{Operating system virtualisation} refers to all mechanisms that enable the creation of secure
and isolated application environments that run on top of a shared kernel. 
Conventionally, these mechanisms are baked into the kernel and are therefore part of the trusted computing base.
The kernel may expose these through its system-call interface, thereby allowing a user-space daemon 
program to provide an automated facility for creating and orchestrating sandboxed environments. 
This is the only feasible architecture on a general-purpose kernel such as Linux. 
Alternatively, the kernel may treat every software component, including its own subsystems, as an entity
to be wrapped in a sandbox. In that case, the concept of a process itself would have to 
satisfy all three virtualisation axioms. Examples of such operating systems include the seL4 \textit{microkernel}
- the first operating system to be formally verified as free of programming errors \cite{10.1145/1629575.1629596}, 
and Google's Fuchsia - described by \textcite{10.22667/JOWUA.2021.09.30.047}.

We first consider operating system virtualisation using a user-space daemon program.
In this architecture, an application, referred to as a \textit{container},
is defined as an encapsulation of \enquote{[...] a software component and all its dependencies 
in a format that is self-describing and portable, so that any compliant container runtime can run it without extra 
dependencies, regardless of the underlying machine and the contents of the container} \cite[1]{oci-runtime-principles}.
A \textit{container runtime} is the user-space daemon program responsible for bringing this portable but 
static representation of an application into execution. In runtime, a container consists of a collection of processes 
that share a restricted view of the system's resources. For example, every container \enquote{believes}
it has a dedicated network stack with its own network interfaces, routing tables and packet-forwarding rules.
All processes in the container can access and manipulate that network stack, but no other process 
outside the container has that capability.
The container runtime configures this invariant and the operating system enforces it by 
namespacing system resources. The Open Containers Initiative (OCI) \cite{oci-website} has 
developed a runtime specification that standardises the operations a container runtime
needs to support. Most importantly, it must allow an external process called a \textit{container engine}
to hook into the lifecycle of a container. The container engine can use these hooks to manage 
all the containers on a single host system. In addition, the engine can attach network and storage 
to containers, thereby allowing processes in different sandboxes to share state and communicate 
with each other, if required. At the highest level of abstraction sits an orchestration platform that 
manages containers on multiple hosts by interacting with the container engine on each system.
This platform constantly monitors node and container health and dynamically multiplexes workloads 
based on various system properties of the cluster as to ensure maximum application availability.

It is important to note that, by definition, the kernel is assumed to be trustworthy. 
It has full control of all hardware resources and can access and modify the execution environment of every process on the system. 
In other words, noninterference between the kernel and user processes is not guaranteed.
Therefore, if the kernel is compromised, all processes on the system become untrustworthy.
It follows that if a process compromises the kernel, it transitively interferes with all other 
processes on the system. Hardening the operating system by implementing various security features such as
mandatory access control has been the primary approach for protection against such scenarios. 
However, the size of a monolithic general-purpose kernel is too large to adequately 
create a threat model that captures all possible vectors of attack. This problem is of particular concern 
to infrastructure providers whose entire business model revolves around consolidating hundreds of potentially 
malicious client applications on the same server, all of which share the same kernel and are allowed to directly interact with 
it via its system call interface. 

Unlike hardware virtualisation, this architecture does not use hardware emulation as an 
isolation primitive.
This means that shadow pages need not be maintained per virtual environment. Input-output operations need only 
traverse the kernel's stack without any address translations and with the additional 
performance benefit of direct memory access. As a result, the isolation overhead is lower 
compared to a virtual machine, which allows more applications to be consolidated onto a single server.
Furthermore, guests do not boot up complete operating system images, which reduces 
start times and memory consumption.
\textcite{10.1145/3126908.3126925} use containers in high-performance 
computing clusters to run user-defined compute jobs and show that the imposed performance penalties 
are, at most, negligible compared to vanilla processes that have no additional isolation.
\textcite{7095802} show the exact same thing and further conclude that the Docker container engine 
is resource-friendlier and faster than the Kernel Virtual Machine (KVM) when stressing the
\enquote{memory, IPC, network and filesystem subsystems} \textcite[1]{7095802} of the Linux kernel 
by running a database inside a virtual environment and evaulating its performance via the SysBench OLTP benchmark \cite{sysbench-oltp}.

Google's gVisor \cite{google-gvisor} attempts to sustain the performance advantages of 
containers whilst introducing an additional isolation boundary between the kernel and each container. The authors implement 
a substantial portion of the system call interface in a user-space process called Sentry.
Their dedicated container runtime calls out to Sentry instead of the kernel when issuing system calls.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.55\textwidth]{images/fundamentals/cont-arch.jpg}
    \caption{Operating system virtualisation architecture using containers. The container runtime starts containers on a single host.
    A user process can see a bundle of resources allocated to it by the kernel. The kernel guarantees that a process 
    cannot see any other resources.
    The container engine manages all containers on a single system and allocates storage and networking to create explicit paths between containers.
    The orchestration platform talks to all engines inside a cluster to provide automatic workload management.}
    \label{images:fundamentals/cont-arch.jpg}
\end{figure}
However, \textcite{234857} show that network and memory allocation performance greatly suffer. 
This can be partially attributed to the fact that the Sentry process is implemented in a garbage-collected language 
and lacks the fine-grained optimisations contained in the Linux kernel.
\textcite{246288} take a different approach and try to fuse the security of virtual machines 
with the performance of containers by programming a custom virtual machine monitor 
called Firecracker that runs on top of KVM. Firecracker completely relies on the
Linux kernel for memory management, CPU scheduling and block I/O. To reduce its attack surface,
the virtual machine monitor sacrifices portability by supporting a limited set of emulated 
network and block devices. To further strengthen the noninterference boundary, the devices 
have configurable built-in rate limiters that can control the number of operations per second, e.g
disk/packets per second. Unlike a traditional container runtime, Firecracker's rate-limiting implementation
does not rely on the kernel, which makes its isolation boundary to the kernel stronger. 
\textcite{10.1145/3381052.3381315} evaluate both gVisor and Firecracker and show that 
the latter \enquote{[...] is effective at reducing the frequency of kernel code invocations, but had 
a much smaller impact on reducing the footprint of kernel code} \cite[12]{10.1145/3381052.3381315}. 
\clearpage

\section{Processes}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/fundamentals/proc-mem-layout.jpg}
    \caption{Memory layout of a process \cite{10.5555/2490781}}
    \label{images:fundamentals/proc-mem-layout}
\end{figure}

A process is the fundamental abstraction of operating-system virtualisation.  
It is the primary unit of work in a computer system and represents a program in execution.
This chapter introduces various aspects of a process - how it is laid out in memory 
(Chapter \ref{ch:fundamentals/processes/memory-layout}), how it is scheduled for execution 
(Chapter \ref{ch:fundamentals/processes/execution-scheduling}), and the system call interface for 
creating, terminating, monitoring and protecting processes from each other 
(Chapters \ref{ch:fundamentals/processes/creation}, \ref{ch:fundamentals/processes/termination},
\ref{ch:fundamentals/processes/monitoring}, \ref{ch:fundamentals/processes/resource-management}, 
\ref{ch:fundamentals/processes/security}).

Every process on Linux is represented as a \verb|task_struct| \cite{include/linux/sched.h} 
which contains various bits of information such as the process's memory descriptor, the table of 
open file descriptors, the running state of the process (see Table \ref{table:proc-states}), 
its pending signals and so on.
Processes are organised in a hierarchy, where each process can have an arbitrary number of children
and a single parent. The kernel expresses this relationship by having each task store a pointer 
to its parent and a circular doubly linked list to all of its children. 

\subsection{Memory Layout}
\label{ch:fundamentals/processes/memory-layout}
The kernel partitions the virtual address space of a process into multipe regions shown in Figure 
\ref{images:fundamentals/proc-mem-layout}. The address space itself is encapsulated in a 
memory descriptor type called \verb|mm_struct| \cite{include/linux/mm_types.h} and 
every process is associated with one or more instances of that type. The text 
segment of the address space is a read-only memory section that contains the set of executable 
instructions that the program compiles down to \cite{10.5555/2490781}. When the program is 
first scheduled for execution, the central processing unit jumps to the text section's 
initial memory address, which is stored in the \verb|start_code| field of the process's memory descriptor
and starts executing from there until it reaches the final linear address \verb|end_code|.
The data section contains globally defined variables that can be referenced throughout the 
entire lifecycle of the process \cite{10.5555/2490781}. The data section is further deconstructed 
into uninitialised and initialised subsections. Similarly to the text section, it
does not grow or shrink because its size is known at program startup and does not change. 
The \verb|start_data| and \verb|end_data| fields in the memory descriptor refer to the 
contiguous memory region that represents the initialised data. 
The heap segment contains memory that is allocated during runtime as per demand. The process 
itself is responsible for releasing any memory acquired from the heap. The latter is important for two 
reasons. First, the heap grows upwards towards the stack and may overlap with it. Second, 
memory allocation interfaces such as \verb|malloc| acquire memory from a system-wide pool. If a 
long-running process acquires dynamic memory but fails to release it, the whole system will 
eventually run out of memory - an example of spatial interference. The kernel tracks the heap areas 
allocated to a particular process via the \verb|start_brk| and \verb|brk| fields of the memory 
descriptor which denote its initial and final linear addresses, respectively. 
The stack is a data structure that acts as temporary storage for a process. Whenever 
a process calls a function, that function's parameters, local variables and return address are pushed 
onto the stack. When the function returns control to the caller, these values are popped of \cite{10.5555/2490781}.
The stack grows downwards towards lower addresses and its starting address \verb|start_stack|
sits right above the \verb|main()| function's return address. 

Note that the virtual address space of the process is sparse because it contains a hole between the 
stack and the heap that will require physical pages only if those two segments grow. This hole may be
utilised to bring in page frames that can be shared between processes in order to reduce memory 
consumption and to allow two processes to communicate with each other. For example, the \verb|libc|
library is reentrant and can be brought into physical memory only once. The memory descriptor of a 
process that depends on \verb|libc| does not have to load its own copy of the library into memory. 
Instead, its page table entries can map to the same physical frames that all other processes refer to 
when using the library. This reduces memory consumption and therefore has positive implications 
on same-host density.

\subsection{Scheduling}
\label{ch:fundamentals/processes/execution-scheduling}
Systems running more than one process rely on a process scheduler to determine, in real-time, 
which process should use the processing unit such that all processes do meaningful work 
and the core does not idle. 
The process scheduler maintains a  queue of all processes that are currently 
being executed by a processing unit or are waiting to be assigned to one. 
This queue is represented in memory as a circular doubly linked-list of \verb|task_struct| 
objects and is called the run queue because all of its tasks are in the \verb|TASK_RUNNING| state.
The kernel keeps a run queue local pointer \verb|current| to the current task being executed by a processing unit. 
It is the scheduler's job to determine the most optimal sequence of processes to schedule from the run queue 
onto the processing unit in order to satisfy two conflicting requirements - low latency and high throughput. 
Low latency is crutial for processes that mostly perform input-output operations. 
Fetching data from a device, such as a network card, typically takes multiple CPU cycles. 
For this reason, the scheduler moves a task currently waiting for an external event from the running queue 
into the wait queue and updates the \verb|current| pointer to point to a different process. 
This operation is known as preemption. 
\begin{table}[h!]
    \centering
    \begin{tabular}{ |m{4cm}|m{20em}| }
        \hline
        State identifier & Description \\
        \hline
        \verb|TASK_RUNNING| & The process is being executed by a CPU or is waiting to be assigned to one \\
        \hline 
        \verb|TASK_UNINTERRUPTIBLE| & The process is waiting for an event or a resource and cannot be interrupted by a signal while doing so \\
        \hline
        \verb|TASK_INTERRUPTIBLE| & The process is waiting for an event or a resource and can be interrupted by a signal while doing so \\
        \hline
        \verb|TASK_NEW| & The process has been created but it cannot be scheduled for execution and it cannot react to events \\
        \hline
        \verb|EXIT_DEAD| & The process is being cleaned up and deleted \\
        \hline
        \verb|EXIT_ZOMBIE| & The process has exited, the parent has been notified via a \verb|SIGCHLD| signal but the parent has not reaped it yet \\
        \hline
    \end{tabular}
    \caption{Table of most important process states in Linux derived from \cite{include/linux/sched.h} and \cite{kernel/sched/core.c}}
    \label{table:proc-states}
\end{table}
The wait queue consists of all processes currently waiting for data or a signal.
Their state is always either \verb|TASK_INTERRUPTIBLE| or \verb|TASK_UNINTERRUPTIBLE|.
In order for an input-output bound process to make meaningful progress, it needs to be frequently 
rescheduled back onto the processing unit. 
Conversely, high throughput is important for processes that are primarily executing computations.
These processes make the most progress by monopolising the processing unit. Hence, 
low-latency and high throughput are incongruous. Operating systems manage 
this incongruity by ranking processes based on their priority and allocating timeslices that 
determine how long a task can run until it is preempted.

On Linux, every \verb|task_struct| is associated with a \verb|sched_entity| object that holds task-related 
statistics used by the scheduler to do its job. The most important properties of this object 
are the \verb|weight| and \verb|vruntime| fields. The former represents 
a priority value that measures the task's willingness to be preempted. In other words, it represents 
how \enquote{nice} a process is to all other processes on the system. The value ranges from $-20$
to $19$ with a default of $0$. Larger values correspond to lower priorities and smaller values correspond 
to higher priorities. The \verb|vruntime| (virtual runtime) field denotes the amount of time already 
allocated to the task. The invariant that the scheduler tries to enforce is that the process with the 
smallest virtual runtime should be the one that is currently using the processing unit. By definition,
the process with the smallest virtual runtime is also the process that has used the processing unit
the least. For this reason, the scheduler is said to be completely fair. 
Whenever the run queue is updated, either by an interrupt triggered by the system timer or a 
change in a task's state, the virtual runtime of the task is incremented by a weighted delta
\begin{equation}
    d = e \times w / \sum_{i = 0}^{n} w_{i}
    \label{equation:fundamentals/cfs-scheduler-delta}
\end{equation}
where $e$ refers to the amount of time the task used the central processing unit 
since the last time the run queue was updated. 
$w$ corresponds to the priority value of the task. $n$ denotes the length of the run queue. 
The sum in the expression aggregates the priority values of all tasks in the queue. 
Hence, dividing $w$ by the sum gives us a proxy of the task's priority relative to all 
other tasks in the queue. Recall that the scheduler monotonically increases the virtual runtime 
of the process by $d$. A process that performs a lot of input-output operations exhibits small 
processing unit bursts, hence $e$ is smaller. Therefore, the virtual runtime for that process 
will increase more slowly and the process will be scheduled more frequently for execution. 
Similarly, if the process has a high priority, i.e $w$ is small, 
the division in Equation \ref{equation:fundamentals/cfs-scheduler-delta} will result in a small fraction 
that will cancel out a big portion of $e$ when multiplied. 

The \verb|sched_entity| object of every process is a part of a 
a red-black tree whose search key is set to the virtual runtime property. The scheduler picks a new process 
for execution by traversing the tree and finding the task with the lowest virtual runtime, which is always 
stored at the leftmost leaf node. The cost of traversal equals the depth of the tree, i.e it takes $\Theta{(\log{n})}$.
However, the scheduler caches the leftmost node, thereby reducing the cost of traversal down to $\Theta{(1)}$.
Whenever a process transitions from the \verb|TASK_NEW| state into the \verb|TASK_RUNNING| state, 
it is assigned the minimal allowed virtual runtime. Therefore, it is automatically 
placed at the leftmost node and gets to execute immediately. As the system progresses, the process will
shift more and more to the right branches of the tree and the rest of the tasks will get to execute.

\subsection{Creation}
\label{ch:fundamentals/processes/creation}
The \verb|clone()| and \verb|fork()| system calls are the only mechanisms for creating a 
new process from user space. The former is a non-portable but very flexible method for creating a
duplicate of the calling process. It gives the caller fine-grained control over the resources that 
the parent and the child get to share. 
The \verb|clone()| system call is used by threading libraries to implement threads. 
This is possible because the kernel does not differentiate between processes and threads. 
In essence, the only difference between both is that threads share the same address space, i.e 
their memory descriptor points to the same virtual memory pages. Additionally, the kernel 
uses this system call to implement \verb|fork()|. 

\subsection{Termination}
\label{ch:fundamentals/processes/termination}
\subsection{Monitoring}
\label{ch:fundamentals/processes/monitoring}
\subsection{Communication}
\label{ch:fundamentals/processes/communication}
\subsection{Resource Management}
\label{ch:fundamentals/processes/resource-management}
\subsection{Security}
\label{ch:fundamentals/processes/security}